{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. load input image and user edit prompt\n",
    "2. operate VLM (Qwen to extract the parameters): id, class, bbox, matrix.\n",
    "3. operate SAM to extract the binary mask\n",
    "4. operate open-CV to to compute the shape mask.\n",
    "5. operate SDL drawer with shape mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load inputs (source image and user prompt)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image as IPImage\n",
    "from PIL import Image\n",
    "\n",
    "# Fixed image path\n",
    "image_path = \"data/elephant.jpg\"\n",
    "\n",
    "# Load the image using PIL\n",
    "try:\n",
    "    source_image = Image.open(image_path)\n",
    "    # Display the image inline in the notebook\n",
    "    display(IPImage(filename=image_path))\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the path and try again.\")\n",
    "\n",
    "# Create a text box widget for the user prompt\n",
    "user_prompt_widget = widgets.Text(\n",
    "    description='User Prompt:',\n",
    "    placeholder='Enter your prompt here'\n",
    ")\n",
    "\n",
    "# Display the widget\n",
    "display(user_prompt_widget)\n",
    "\n",
    "# Function to print the user prompt\n",
    "def print_user_prompt(change):\n",
    "    user_prompt = user_prompt_widget.value\n",
    "    print(f\"User prompt: {user_prompt}\")\n",
    "\n",
    "# Attach the function to the user prompt widget to trigger on 'submit'\n",
    "user_prompt_widget.on_submit(print_user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QWEN-VL-1 (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use QWEN-VL to parse the parameters\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "import torch\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Note: The default behavior now has injection attack prevention off.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n",
    "\n",
    "# use bf16\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n",
    "# use fp16\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n",
    "# use cpu only\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n",
    "# use cuda device with bf16\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True, bf16=True).eval()\n",
    "\n",
    "# Specify hyperparameters for generation\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n",
    "\n",
    "# 1st dialogue turn\n",
    "query = tokenizer.from_list_format([\n",
    "    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url\n",
    "    {'text': '这是什么?'},\n",
    "])\n",
    "response, history = model.chat(tokenizer, query=query, history=None)\n",
    "print(response)\n",
    "# 图中是一名女子在沙滩上和狗玩耍，旁边是一只拉布拉多犬，它们处于沙滩上。\n",
    "\n",
    "# 2nd dialogue turn\n",
    "response, history = model.chat(tokenizer, '框出图中击掌的位置', history=history)\n",
    "print(response)\n",
    "# <ref>击掌</ref><box>(536,509),(588,602)</box>\n",
    "image = tokenizer.draw_bbox_on_latest_picture(response, history)\n",
    "if image:\n",
    "  image.save('1.jpg')\n",
    "else:\n",
    "  print(\"no box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QWEN2 EXample (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/QwenLM/Qwen2.5-Math\n",
    "\n",
    " \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Math-72B-Instruct\"\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Find the value of $x$ that satisfies the equation $4x+5 = 6x+7$.\"\n",
    "\n",
    "# CoT\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# TIR\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Response:\", response)\n",
    "print(\"completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Image Attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and resize input image\n",
    "IMG_PATH = './data/demo.jpeg'\n",
    "############################################################\n",
    "\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "# Add debug print to check if image file exists\n",
    "if not os.path.exists(IMG_PATH):\n",
    "    print(f\"ERROR: Image file not found at {IMG_PATH}\")\n",
    "\n",
    "# Load model with flash attention for better performance\n",
    "try:\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading model: {str(e)}\")\n",
    "\n",
    "# Load processor\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading processor: {str(e)}\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are a computer vision expert specializing in object detection, scene understanding and spatial relationships.\n",
    "For each detected object in the image, output exactly one line in the following format:\n",
    "DETECT: <object_id>|<object_class>|<xmin>|<ymin>|<xmax>|<ymax>\n",
    "\n",
    "Then, provide a scene analysis in the following format:\n",
    "SCENE: <scene_description>\n",
    "\n",
    "Finally, describe spatial relationships between objects:\n",
    "SPATIAL: <spatial_relationships>\n",
    "\n",
    "Where:\n",
    "- <object_id> is a sequential number starting from 1\n",
    "- <object_class> is the specific object category including attributes (color, size, etc)\n",
    "- <xmin>,<ymin>,<xmax>,<ymax> are normalized coordinates (0-1) for the bounding box\n",
    "- <scene_description> describes the overall context and setting\n",
    "- <spatial_relationships> describes how objects relate to each other in space\n",
    "\n",
    "Focus on main objects, and their relationships.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": IMG_PATH\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Analyze this image, detect the main objects, describe the scene and spatial relationships between objects.\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process inputs\n",
    "try:\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR in input processing: {str(e)}\")\n",
    "\n",
    "# Run inference\n",
    "try:\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=256)  # Increased tokens for scene description\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"ERROR in model inference: {str(e)}\")\n",
    "\n",
    "# Parse the detection and scene understanding output\n",
    "def parse_model_output(output_text):\n",
    "    bboxes = []\n",
    "    classes = []\n",
    "    scene_desc = \"\"\n",
    "    spatial_rel = \"\"\n",
    "    \n",
    "    try:\n",
    "        if isinstance(output_text, list):\n",
    "            output_text = output_text[0]\n",
    "            \n",
    "        for line in output_text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('DETECT:'):\n",
    "                _, detection = line.split(':', 1)\n",
    "                obj_id, obj_class, xmin, ymin, xmax, ymax = detection.strip().split('|')\n",
    "                bbox = [float(xmin), float(ymin), float(xmax), float(ymax)]\n",
    "                bboxes.append(bbox)\n",
    "                classes.append(obj_class)\n",
    "            elif line.startswith('SCENE:'):\n",
    "                _, scene_desc = line.split(':', 1)\n",
    "            elif line.startswith('SPATIAL:'):\n",
    "                _, spatial_rel = line.split(':', 1)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in parse_model_output: {str(e)}\")\n",
    "        \n",
    "    return bboxes, classes, scene_desc.strip(), spatial_rel.strip()\n",
    "\n",
    "# Parse outputs\n",
    "bboxes, classes, scene_desc, spatial_rel = parse_model_output(output_text)\n",
    "\n",
    "def annotate_image_with_bbox(image, bbox, label, color=(0, 255, 0), thickness=2):\n",
    "    try:\n",
    "        height, width = image.shape[:2]\n",
    "        xmin = int(bbox[0] * width)\n",
    "        ymin = int(bbox[1] * height)\n",
    "        xmax = int(bbox[2] * width)\n",
    "        ymax = int(bbox[3] * height)\n",
    "        \n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, thickness)\n",
    "        cv2.putText(image, label, (xmin, ymin-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, thickness)\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in annotate_image_with_bbox: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "# Load and annotate image\n",
    "try:\n",
    "    image = cv2.imread(IMG_PATH)\n",
    "    if image is None:\n",
    "        raise ValueError(\"Could not load image for annotation\")\n",
    "\n",
    "    # Create figure with two subplots\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Annotated image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i, (bbox, class_name) in enumerate(zip(bboxes, classes)):\n",
    "        label = f\"{i+1}: {class_name}\"\n",
    "        image = annotate_image_with_bbox(image, bbox, label)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Detected Objects\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Plot 2: Scene description and spatial relationships\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.text(0.1, 0.7, f\"Scene Description:\\n{scene_desc}\", wrap=True)\n",
    "    plt.text(0.1, 0.3, f\"Spatial Relationships:\\n{spatial_rel}\", wrap=True)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"ERROR in annotation/display: {str(e)}\")\n",
    "\n",
    "\n",
    "# PRINT______________________________________________________   \n",
    "# Print information in a clear format\n",
    "print(\"\\n=== DETECTION RESULTS ===\")\n",
    "print(\"\\nDetected Objects:\")\n",
    "for i, (bbox, class_name) in enumerate(zip(bboxes, classes)):\n",
    "    print(f\"Object {i+1}:\")\n",
    "    print(f\"  Class: {class_name}\")\n",
    "    print(f\"  Bounding Box (normalized): xmin={bbox[0]:.3f}, ymin={bbox[1]:.3f}, xmax={bbox[2]:.3f}, ymax={bbox[3]:.3f}\")\n",
    "\n",
    "print(\"\\nScene Description:\")\n",
    "print(scene_desc)\n",
    "\n",
    "print(\"\\nSpatial Relationships:\")\n",
    "print(spatial_rel)\n",
    "print(\"\\n=====================\")\n",
    "\n",
    "# Save detection results to file\n",
    "try:\n",
    "    img_name = IMG_PATH.split('/')[-1].split('.')[0]\n",
    "    output_file = f\"{img_name}_detection.txt\"\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"=== DETECTION RESULTS ===\\n\")\n",
    "        f.write(\"\\nDetected Objects:\\n\")\n",
    "        for i, (bbox, class_name) in enumerate(zip(bboxes, classes)):\n",
    "            f.write(f\"Object {i+1}:\\n\")\n",
    "            f.write(f\"  Class: {class_name}\\n\")\n",
    "            f.write(f\"  Bounding Box (normalized): xmin={bbox[0]:.3f}, ymin={bbox[1]:.3f}, xmax={bbox[2]:.3f}, ymax={bbox[3]:.3f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nScene Description:\\n\")\n",
    "        f.write(f\"{scene_desc}\\n\")\n",
    "        \n",
    "        f.write(\"\\nSpatial Relationships:\\n\") \n",
    "        f.write(f\"{spatial_rel}\\n\")\n",
    "        f.write(\"\\n=====================\\n\")\n",
    "        \n",
    "    print(f\"\\nDetection results saved to {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saving detection results: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QWEN MATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_FILE=\"data/img_detection.txt\"\n",
    "IMG_PATH = './data/demo.jpeg'\n",
    "############################################################\n",
    "# Read detection results and parse object properties\n",
    "with open(TXT_FILE, 'r') as f:\n",
    "    detection_data = f.read()\n",
    "\n",
    "# Parse objects with their properties\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Extract object information with properties\n",
    "objects = []\n",
    "for match in re.finditer(r'Object (\\d+):\\n\\s+Class: (.*?)\\n\\s+Bounding Box.*?xmin=([\\d.]+), ymin=([\\d.]+), xmax=([\\d.]+), ymax=([\\d.]+)', detection_data):\n",
    "    # Get bounding box coordinates\n",
    "    xmin = float(match.group(3))\n",
    "    ymin = float(match.group(4))\n",
    "    xmax = float(match.group(5))\n",
    "    ymax = float(match.group(6))\n",
    "    \n",
    "    # Calculate corner points of bounding box\n",
    "    corners = np.array([\n",
    "        [xmin, ymin, 1],  # top-left\n",
    "        [xmax, ymin, 1],  # top-right\n",
    "        [xmax, ymax, 1],  # bottom-right\n",
    "        [xmin, ymax, 1]   # bottom-left\n",
    "    ])\n",
    "    \n",
    "    obj = {\n",
    "        'id': int(match.group(1)),\n",
    "        'class': match.group(2),\n",
    "        'bbox': [xmin, ymin, xmax, ymax],\n",
    "        'corners': corners,\n",
    "        'width': xmax - xmin,\n",
    "        'height': ymax - ymin,\n",
    "        'center': [(xmax + xmin)/2, (ymax + ymin)/2]\n",
    "    }\n",
    "    objects.append(obj)\n",
    "\n",
    "# Define the user's edit request\n",
    "USER_EDIT = \"Translate the dog 100 pixels to the right.\"\n",
    "\n",
    "# Define available transformations and their effects on points\n",
    "TRANSFORMATIONS = {\n",
    "    'affine': {\n",
    "        'translation': lambda tx, ty: np.array([\n",
    "            [1, 0, tx],\n",
    "            [0, 1, ty],\n",
    "            [0, 0, 1]\n",
    "        ]),\n",
    "        'rotation': lambda theta: np.array([\n",
    "            [np.cos(theta), -np.sin(theta), 0],\n",
    "            [np.sin(theta), np.cos(theta), 0],\n",
    "            [0, 0, 1]\n",
    "        ]),\n",
    "        'scaling': lambda sx, sy: np.array([\n",
    "            [sx, 0, 0],\n",
    "            [0, sy, 0],\n",
    "            [0, 0, 1]\n",
    "        ]),\n",
    "        'shear': lambda shx, shy: np.array([\n",
    "            [1, shx, 0],\n",
    "            [shy, 1, 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "    },\n",
    "    'non_affine': {\n",
    "        'perspective': 'Custom 3x3 homography matrix',\n",
    "        'deformation': 'Non-linear point mapping function'\n",
    "    }\n",
    "}\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "device = \"cuda\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Construct prompt for transformation inference\n",
    "prompt = f\"\"\"Given the scene with these detected objects and their bounding boxes:\n",
    "{', '.join([f\"ID {obj['id']}: {obj['class']} at coordinates (xmin={obj['bbox'][0]:.3f}, ymin={obj['bbox'][1]:.3f}, xmax={obj['bbox'][2]:.3f}, ymax={obj['bbox'][3]:.3f})\" for obj in objects])}\n",
    "\n",
    "And the user's edit request:\n",
    "{USER_EDIT}\n",
    "\n",
    "Available transformations that can be applied to any point (x,y,1) in homogeneous coordinates:\n",
    "1. Translation: [x,y,1] → [x+tx, y+ty, 1]\n",
    "2. Rotation: [x,y,1] → [x*cos(θ)-y*sin(θ), x*sin(θ)+y*cos(θ), 1]\n",
    "3. Scaling: [x,y,1] → [sx*x, sy*y, 1]\n",
    "4. Shear: [x,y,1] → [x+shx*y, shy*x+y, 1]\n",
    "5. Perspective: [x,y,1] → [h11*x+h12*y+h13, h21*x+h22*y+h23, h31*x+h32*y+h33]\n",
    "\n",
    "Calculate a transformation matrix that will map every point in the target object's bounding box to its new position.\n",
    "Reason step by step about:\n",
    "1. Which object needs to be transformed\n",
    "2. What type of transformation is needed\n",
    "3. The exact parameters required\n",
    "4. The resulting 3x3 transformation matrix\n",
    "\n",
    "Output the final transformation matrix that can be applied to any point [x,y,1] in the bounding box.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a computer vision transformation expert. Calculate precise transformation matrices that can be applied to any point in homogeneous coordinates.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "reasoning = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"\\n=== Model Reasoning ===\")\n",
    "print(reasoning)\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Extract the content inside \\boxed{ }\n",
    "matrix_content = re.search(r\"\\\\boxed\\{\\\\begin\\{pmatrix\\}([\\s\\S]*?)\\\\end\\{pmatrix\\}\\}\", reasoning).group(1)\n",
    "\n",
    "# Parse the content into a 2D list\n",
    "matrix_rows = matrix_content.strip().split(r\"\\\\\")\n",
    "matrix = [list(map(float, row.split('&'))) for row in matrix_rows]\n",
    "\n",
    "# Convert to a NumPy array for better display\n",
    "matrix_array = np.array(matrix)\n",
    "\n",
    "# Print the matrix nicely\n",
    "print(\"Parsed Matrix:\")\n",
    "print(matrix_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM gets binary mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.585 0.545]\n",
      " [0.375 0.63 ]]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "IMG_PATH = './data/demo.jpeg'\n",
    "\n",
    "import torch\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Parse bounding box coordinates from detection file\n",
    "with open('data/demo_detection.txt', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Extract bounding box coordinates for each object\n",
    "objects = []\n",
    "for obj in content.split('Object')[1:]:\n",
    "    # Check if the object has the required fields\n",
    "    if 'Class:' in obj and 'Bounding Box (normalized):' in obj:\n",
    "        class_name = obj.split('Class:')[1].split('\\n')[0].strip()\n",
    "        bbox = obj.split('Bounding Box (normalized):')[1].split('\\n')[0].strip()\n",
    "        \n",
    "        # Initialize bounding box coordinates\n",
    "        xmin = ymin = xmax = ymax = None\n",
    "        \n",
    "        # Extract bounding box coordinates safely\n",
    "        for coord in bbox.split(','):\n",
    "            if 'xmin' in coord:\n",
    "                xmin = float(coord.split('=')[1].strip())\n",
    "            elif 'ymin' in coord:\n",
    "                ymin = float(coord.split('=')[1].strip())\n",
    "            elif 'xmax' in coord:\n",
    "                xmax = float(coord.split('=')[1].strip())\n",
    "            elif 'ymax' in coord:\n",
    "                ymax = float(coord.split('=')[1].strip())\n",
    "        \n",
    "        # Ensure all coordinates are found before calculating center\n",
    "        if None not in (xmin, ymin, xmax, ymax):\n",
    "            # Calculate center point\n",
    "            center_x = (xmin + xmax) / 2\n",
    "            center_y = (ymin + ymax) / 2\n",
    "            \n",
    "            objects.append({\n",
    "                'class': class_name,\n",
    "                'center': [center_x, center_y]\n",
    "            })\n",
    "\n",
    "# Convert to input format for SAM\n",
    "input_points = np.array([[obj['center'][0], obj['center'][1]] for obj in objects])\n",
    "input_labels = np.array([1] * len(objects))  # 1 indicates foreground point\n",
    "\n",
    "print(input_points)\n",
    "print(input_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "#     predictor.set_image(IMG_PATH)\n",
    "#     masks, scores, _ = predictor.predict(\n",
    "#         point_coords=input_points,\n",
    "#         point_labels=input_labels,\n",
    "#         multimask_output=False\n",
    "#     )\n",
    "\n",
    "# # Visualize the results\n",
    "# plt.figure(figsize=(15, 10))\n",
    "\n",
    "# # Plot original image\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(IMG_PATH)\n",
    "# plt.title('Original Image')\n",
    "# plt.axis('off')\n",
    "\n",
    "# # Plot image with masks overlay\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(IMG_PATH)\n",
    "# for i, mask in enumerate(masks):\n",
    "#     plt.imshow(mask, alpha=0.5, cmap='jet')\n",
    "#     plt.plot(input_points[i][0], input_points[i][1], 'r+', markersize=10)\n",
    "# plt.title('Segmentation Masks')\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/dtu/blackhole/14/189044/marscho/VLM_controller_for_SD/.venv/lib/python3.10/site-packages/sam2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/dtu/blackhole/14/189044/marscho/VLM_controller_for_SD/sam2/checkpoints/sam2.1_hiera_large.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m model_cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/dtu/blackhole/14/189044/marscho/VLM_controller_for_SD/sam2/sam2/configs/sam2.1/sam2.1_hiera_l.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m predictor \u001b[38;5;241m=\u001b[39m SAM2ImagePredictor(\u001b[43mbuild_sam2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/dtu/blackhole/14/189044/marscho/VLM_controller_for_SD/.venv/lib/python3.10/site-packages/sam2/build_sam.py:90\u001b[0m, in \u001b[0;36mbuild_sam2\u001b[0;34m(config_file, ckpt_path, device, mode, hydra_overrides_extra, apply_postprocessing, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m     hydra_overrides_extra \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;66;03m# dynamically fall back to multi-mask if the single mask is not stable\u001b[39;00m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m++model.sam_mask_decoder_extra_args.dynamic_multimask_via_stability=true\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m++model.sam_mask_decoder_extra_args.dynamic_multimask_stability_delta=0.05\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m++model.sam_mask_decoder_extra_args.dynamic_multimask_stability_thresh=0.98\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     88\u001b[0m     ]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Read config and init model\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[43mcompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhydra_overrides_extra\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m OmegaConf\u001b[38;5;241m.\u001b[39mresolve(cfg)\n\u001b[1;32m     92\u001b[0m model \u001b[38;5;241m=\u001b[39m instantiate(cfg\u001b[38;5;241m.\u001b[39mmodel, _recursive_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/dtu/blackhole/14/189044/marscho/VLM_controller_for_SD/.venv/lib/python3.10/site-packages/hydra/compose.py:38\u001b[0m, in \u001b[0;36mcompose\u001b[0;34m(config_name, overrides, return_hydra_config, strict)\u001b[0m\n\u001b[1;32m     36\u001b[0m gh \u001b[38;5;241m=\u001b[39m GlobalHydra\u001b[38;5;241m.\u001b[39minstance()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m gh\u001b[38;5;241m.\u001b[39mhydra \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[43mgh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhydra\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRunMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRUN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_shell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_log_configuration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cfg, DictConfig)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_hydra_config:\n",
      "File \u001b[0;32m/dtu/blackhole/14/189044/marscho/VLM_controller_for_SD/.venv/lib/python3.10/site-packages/hydra/_internal/hydra.py:594\u001b[0m, in \u001b[0;36mHydra.compose_config\u001b[0;34m(self, config_name, overrides, run_mode, with_log_configuration, from_shell, validate_sweep_overrides)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompose_config\u001b[39m(\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    578\u001b[0m     config_name: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    583\u001b[0m     validate_sweep_overrides: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    584\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DictConfig:\n\u001b[1;32m    585\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;124;03m    :param config_name:\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;124;03m    :param overrides:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_configuration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_shell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_shell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_sweep_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_sweep_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m with_log_configuration:\n\u001b[1;32m    602\u001b[0m         configure_log(cfg\u001b[38;5;241m.\u001b[39mhydra\u001b[38;5;241m.\u001b[39mhydra_logging, cfg\u001b[38;5;241m.\u001b[39mhydra\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[0;32m/dtu/blackhole/14/189044/marscho/VLM_controller_for_SD/.venv/lib/python3.10/site-packages/hydra/_internal/config_loader_impl.py:142\u001b[0m, in \u001b[0;36mConfigLoaderImpl.load_configuration\u001b[0;34m(self, config_name, overrides, run_mode, from_shell, validate_sweep_overrides)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_configuration\u001b[39m(\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    135\u001b[0m     config_name: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     validate_sweep_overrides: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DictConfig:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_configuration_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfrom_shell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_shell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_sweep_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_sweep_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m OmegaConfBaseException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConfigCompositionException()\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/dtu/blackhole/14/189044/marscho/VLM_controller_for_SD/.venv/lib/python3.10/site-packages/hydra/_internal/config_loader_impl.py:243\u001b[0m, in \u001b[0;36mConfigLoaderImpl._load_configuration_impl\u001b[0;34m(self, config_name, overrides, run_mode, from_shell, validate_sweep_overrides)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load_configuration_impl\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     config_name: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m     validate_sweep_overrides: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DictConfig:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhydra\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__, version\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_main_config_source_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     parsed_overrides, caching_repo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_overrides_and_create_caching_repo(\n\u001b[1;32m    245\u001b[0m         config_name, overrides\n\u001b[1;32m    246\u001b[0m     )\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validate_sweep_overrides:\n",
      "File \u001b[0;32m/dtu/blackhole/14/189044/marscho/VLM_controller_for_SD/.venv/lib/python3.10/site-packages/hydra/_internal/config_loader_impl.py:110\u001b[0m, in \u001b[0;36mConfigLoaderImpl.ensure_main_config_source_available\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sources():\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# if specified, make sure main config search path exists\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m source\u001b[38;5;241m.\u001b[39mprovider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 110\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavailable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    111\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m source\u001b[38;5;241m.\u001b[39mscheme() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpkg\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    112\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m source\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/dtu/blackhole/14/189044/marscho/VLM_controller_for_SD/.venv/lib/python3.10/site-packages/hydra/_internal/core_plugins/importlib_resources_config_source.py:67\u001b[0m, in \u001b[0;36mImportlibResourcesConfigSource.available\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__init__.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/blackhole/14/189044/marscho/VLM_controller_for_SD/.venv/lib/python3.10/site-packages/hydra/_internal/core_plugins/importlib_resources_config_source.py:67\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__init__.py\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mis_file() \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files\u001b[38;5;241m.\u001b[39miterdir())\n",
      "File \u001b[0;32m/appl9/python/3.10.12/lib/python3.10/pathlib.py:1017\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Iterate over the files in this directory.  Does not yield any\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;124;03m    result for the special paths '.' and '..'.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m}:\n\u001b[1;32m   1019\u001b[0m             \u001b[38;5;66;03m# Yielding a path object for these makes little sense\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/dtu/blackhole/14/189044/marscho/VLM_controller_for_SD/.venv/lib/python3.10/site-packages/sam2'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "checkpoint = \"/dtu/blackhole/14/189044/marscho/VLM_controller_for_SD/sam2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"/dtu/blackhole/14/189044/marscho/VLM_controller_for_SD/sam2/sam2/configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
